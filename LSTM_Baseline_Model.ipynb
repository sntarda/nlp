{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re"
      ],
      "metadata": {
        "id": "H-sSgdRCrCq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook display configuration\n",
        "from IPython.display import HTML, display\n",
        "def set_css():\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "      pre { white-space: pre-wrap; }\n",
        "    </style>\n",
        "    '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "uYpfsyPfpjVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "k9T7i8GNPHqG",
        "outputId": "f0fabf1b-43db-437b-cb3f-2b4ea321b404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_path = '/content/drive/MyDrive/Artificial Intelligence/Class: Natural Language Processing NLP/Project /Phase 1/Output/QuranDF_wTokensTags.xlsx'\n",
        "QURAN_DF = pd.read_excel(df_path)\n",
        "QURAN_DF['Tokens'] = QURAN_DF['Tokens'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "92AZFIJ0rNdd",
        "outputId": "6d88f1f7-7340-4414-a2e5-9139ff87e53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation for Modeling**\n",
        "\n",
        "1.   **Data Consolidation**\n",
        "\n",
        "      **- Generate Input (X) and Target (Y) Sequences**"
      ],
      "metadata": {
        "id": "qBkpNtsQQUCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Input (X) and Target (Y) Sequences\n",
        "X = []\n",
        "Y = []\n",
        "for tokens_list in QURAN_DF['Tokens']:\n",
        "    for i in range(1, len(tokens_list)):\n",
        "        X.append(' '.join(tokens_list[:i]))\n",
        "        Y.append(tokens_list[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XZRKLwx5QU-d",
        "outputId": "7b311627-5823-49e2-ef35-7db36bbd3abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X: {X[:5]}\")\n",
        "print(f\"Y: {Y[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KaoZ__wcsF9k",
        "outputId": "adc327b8-401c-4cfd-a07b-d7278483baf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: ['بِسْمِ', 'بِسْمِ اللَّهِ', 'بِسْمِ اللَّهِ الرَّحْمَـٰنِ', 'الْحَمْدُ', 'الْحَمْدُ لِلَّهِ']\n",
            "Y: ['اللَّهِ', 'الرَّحْمَـٰنِ', 'الرَّحِيمِ', 'لِلَّهِ', 'رَبِّ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "# Display the first few sequences and their corresponding targets\n",
        "for i in range(5):\n",
        "    print(f\"X[{i}]: {X[i]}\")\n",
        "    print(f\"Y[{i}]: {Y[i]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "3S-R-Qv3sfRo",
        "outputId": "de3b3b07-4238-444b-c81b-6453902f4416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X[0]: بِسْمِ\n",
            "Y[0]: اللَّهِ\n",
            "\n",
            "X[1]: بِسْمِ اللَّهِ\n",
            "Y[1]: الرَّحْمَـٰنِ\n",
            "\n",
            "X[2]: بِسْمِ اللَّهِ الرَّحْمَـٰنِ\n",
            "Y[2]: الرَّحِيمِ\n",
            "\n",
            "X[3]: الْحَمْدُ\n",
            "Y[3]: لِلَّهِ\n",
            "\n",
            "X[4]: الْحَمْدُ لِلَّهِ\n",
            "Y[4]: رَبِّ\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer and Sequences Preparation**\n",
        "\n"
      ],
      "metadata": {
        "id": "0emLd9p5s2mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Since 'X' is a list of lists (each inner list being a sequence of tokens),\n",
        "# and 'Y' is a list of next tokens, you need to flatten 'X' for fitting\n",
        "# and prepare 'Y' as a list of texts as well.\n",
        "all_sequences = [' '.join(seq) for seq in X] + [' '.join([y]) for y in Y]  # Convert Y tokens to strings for uniformity\n",
        "\n",
        "# Fit the tokenizer on your sequences\n",
        "tokenizer.fit_on_texts(all_sequences)\n",
        "\n",
        "# Convert tokens to sequences\n",
        "X_seq = tokenizer.texts_to_sequences([' '.join(seq) for seq in X])\n",
        "Y_seq = tokenizer.texts_to_sequences([' '.join([y]) for y in Y])\n",
        "\n",
        "# Since 'Y' is essentially a single token (next word) for each sequence,\n",
        "# and we've converted it to a sequence of integers (which will be just one integer),\n",
        "# we need to flatten 'Y_seq' to be a 1D array.\n",
        "Y_seq = [seq[0] for seq in Y_seq]  # Flatten Y_seq\n",
        "\n",
        "# Pad sequences for 'X'\n",
        "X_padded = pad_sequences(X_seq, padding='post')\n",
        "\n",
        "# 'Y' doesn't need padding since it's a single token per sequence,\n",
        "# but make sure it's correctly shaped for training.\n",
        "\n",
        "print(f\"Shape of X_padded: {X_padded.shape}\")\n",
        "print(f\"Length of Y_seq: {len(Y_seq)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "n1eFLhgfQkhb",
        "outputId": "2afedd0b-9615-4dcf-8b16-a39fb2ab21d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_padded: (5875, 1049)\n",
            "Length of Y_seq: 5875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few sequences\n",
        "for i in range(3):\n",
        "    print(f\"Original X[{i}]: {X[i]}\")\n",
        "    print(f\"Encoded X_seq[{i}]: {X_seq[i]}\")\n",
        "    print(f\"Padded X_padded[{i}]: {X_padded[i]}\")\n",
        "    print(f\"Original Y[{i}]: {Y[i]}\")\n",
        "    print(f\"Encoded Y_seq[{i}]: {Y_seq[i]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "a7H5hNJCQqkC",
        "outputId": "63088fb9-a525-479f-99a6-3eb3843601e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original X[0]: بِسْمِ\n",
            "Encoded X_seq[0]: [14, 3, 21, 5, 7, 3]\n",
            "Padded X_padded[0]: [14  3 21 ...  0  0  0]\n",
            "Original Y[0]: اللَّهِ\n",
            "Encoded Y_seq[0]: 49\n",
            "\n",
            "Original X[1]: بِسْمِ اللَّهِ\n",
            "Encoded X_seq[1]: [14, 3, 21, 5, 7, 3, 2, 4, 4, 10, 1, 12, 3]\n",
            "Padded X_padded[1]: [14  3 21 ...  0  0  0]\n",
            "Original Y[1]: الرَّحْمَـٰنِ\n",
            "Encoded Y_seq[1]: 447\n",
            "\n",
            "Original X[2]: بِسْمِ اللَّهِ الرَّحْمَـٰنِ\n",
            "Encoded X_seq[2]: [14, 3, 21, 5, 7, 3, 2, 4, 4, 10, 1, 12, 3, 2, 4, 15, 10, 1, 27, 5, 7, 1, 23, 18, 9, 3]\n",
            "Padded X_padded[2]: [14  3 21 ...  0  0  0]\n",
            "Original Y[2]: الرَّحِيمِ\n",
            "Encoded Y_seq[2]: 305\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Assuming X_padded contains your input sequences and Y_seq the corresponding next tokens\n",
        "# Splitting the dataset into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_padded, Y_seq, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train)\n",
        "Y_train = torch.tensor(Y_train)\n",
        "X_val = torch.tensor(X_val)\n",
        "Y_val = torch.tensor(Y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "HQ7K67fVRQ3F",
        "outputId": "7376360c-e5f2-4a58-fc9a-923888ca0f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Data Loaders**"
      ],
      "metadata": {
        "id": "88-rV2GlRQX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 10  # Adjust based on your GPU memory\n",
        "\n",
        "# Assuming Y_train and Y_val contain the index of the next token to predict\n",
        "def expand_labels_for_mlm(input_ids, labels_indices, ignore_index=-100):\n",
        "    # Create a labels tensor filled with ignore_index\n",
        "    labels = torch.full(input_ids.shape, ignore_index)\n",
        "    for i, label_index in enumerate(labels_indices):\n",
        "        # Set the label for the position we want to predict\n",
        "        if label_index < input_ids.shape[1]:  # Ensure the index is within the sequence length\n",
        "            labels[i, label_index] = input_ids[i, label_index]\n",
        "    return labels\n",
        "\n",
        "# Example usage\n",
        "labels_train = expand_labels_for_mlm(X_train, Y_train)\n",
        "labels_val = expand_labels_for_mlm(X_val, Y_val)\n",
        "\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_data = TensorDataset(X_train, Y_train)\n",
        "val_data = TensorDataset(X_val, Y_val)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5se5rm2jReY8",
        "outputId": "51df99ad-58fd-4371-f4d2-1d767e1a2785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Collect all word tokens\n",
        "All_Tokens = list(QURAN_DF['Tokens'])\n",
        "All_vocab_list = []\n",
        "\n",
        "for token_list in All_Tokens:\n",
        "    for token in token_list:\n",
        "        # Remove non-Arabic characters from the token\n",
        "        cleaned_token = re.sub(r'[^\\u0600-\\u06FF\\s]', '', token)\n",
        "        # Check if the cleaned token is not empty\n",
        "        if cleaned_token.strip():\n",
        "            if cleaned_token not in All_vocab_list:\n",
        "                All_vocab_list.append(cleaned_token)\n",
        "\n",
        "vocab_size = len(All_vocab_list)\n",
        "print(f\"Vocab Size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "aRD3POZioe7D",
        "outputId": "47793295-131f-48cd-9fcd-5a933c381ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 2757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build Baseline Model**"
      ],
      "metadata": {
        "id": "CZAcGm7KQv6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMBaselineModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LSTMBaselineModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embed the input sequence\n",
        "        embeds = self.embedding(x)\n",
        "        # Pass through the LSTM\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        # Get the last hidden state\n",
        "        last_hidden = lstm_out[:, -1, :]\n",
        "        # Pass the last hidden state through the linear layer\n",
        "        output = self.linear(last_hidden)\n",
        "        return output\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "\n",
        "model = LSTMBaselineModel(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ifrIST7pX3PZ",
        "outputId": "f85d643a-c78c-4035-c62f-c81dee5f56e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMBaselineModel(\n",
              "  (embedding): Embedding(2757, 100)\n",
              "  (lstm): LSTM(100, 128, batch_first=True)\n",
              "  (linear): Linear(in_features=128, out_features=2757, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your DataLoader is set up and called `data_loader`\n",
        "for epoch in range(1):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader :\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_dataloader )}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "M67mgyuEXfiA",
        "outputId": "728d0aa5-d07b-4794-e18c-cef5e4c8674c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.702776829551884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "    correct = (y_true == y_pred).float()  # convert into float for division\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "# Function for making predictions and evaluating model\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_accuracy = 0\n",
        "    total_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Convert outputs to probabilities and predict class with highest probability\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            predictions = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracy = calculate_accuracy(labels, predictions)\n",
        "            total_accuracy += accuracy.item()\n",
        "            total_count += 1\n",
        "\n",
        "    # Calculate average accuracy across all batches\n",
        "    avg_accuracy = total_accuracy / total_count\n",
        "    return avg_accuracy\n",
        "\n",
        "# Assuming you have a validation DataLoader called `val_dataloader`\n",
        "val_accuracy = evaluate_model(model, val_dataloader, device)\n",
        "print(f'Validation Accuracy: {val_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "U2jF6IdEfvnf",
        "outputId": "ae4438a4-256c-4cb0-9e4d-62782f47472a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre { white-space: pre-wrap; }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.0051\n"
          ]
        }
      ]
    }
  ]
}